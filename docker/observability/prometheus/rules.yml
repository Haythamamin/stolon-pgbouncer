---
groups:
  - name: stolon-pgbouncer.rules
    rules:
      - alert: StolonPgBouncerMultipleMasters
        # Count the number of distinct (cluster_name, keeper) label pairs we
        # have, then roll-up by cluster_name. This tells us if the same cluster
        # is pointing at multiple keepers. Use the last_reload metric to ensure
        # the keeper value is what we've successfully loaded PgBouncer to point
        # at.
        expr: >
          count by (cluster_name) (
            count by (cluster_name, keeper) (
              stolon_cluster_identifier * ignoring(cluster_name, component, keeper) group_left(keeper) stolon_pgbouncer_last_reload_seconds
            )
          ) > 1
        labels:
          severity: critical
        annotations:
          summary: PgBouncers in this cluster are pointed at multiple masters
          dashboard: &dashboardStolonPgBouncer viJqeQkWk
          description: |

            The {{ $labels.cluster_name }} cluster has PgBouncers that are pointed at more
            than one keeper. This should still be safe, as connections to replicas will be
            proxied to the master by stolon-proxy. However we should clean up these
            connections as we'll be bypassing the pooling restrictions imposed by the
            master PgBouncer.

            Jump into the dashboard to figure out which pod has gone wrong - it
            will likely be highlighted in red, and won't have reloaded PgBouncer
            in some time.

            To gracefully drain all connections from this pod, just delete it. It'll wait
            up to 3 hours before shutting down to make sure all connections have closed.
            If the new pod comes back and doesn't point itself to the master then you need
            to debug etcd or the cluster itself.

      - alert: StolonPgBouncerPendingShutdown
        expr: >
          max by (instance, namespace, version) (
            stolon_cluster_identifier * ignoring(cluster_name, component) (
              time() - (stolon_pgbouncer_shutdown_seconds > 0)
            ) > 180
          )
        labels:
          severity: warning
        annotations:
          summary: PgBouncer is pending shutdown on {{ $labels.instance }}
          dashboard: *dashboardStolonPgBouncer
          description: |

            stolon-pgbouncer has been sent a SIGTERM and is attempting
            to shutdown, but has been unable to. If this is not handled before
            the termination grace period expires (usually 3hrs) then connections
            will be terminated, causing interruption of service.

            This will happen if you deploy new PgBouncer pods and fail to roll
            the application deployments that use session pooled connections
            through them. If you haven't yet rolled services like
            payments-service, this should be your first action.

            Tail the pod logs to check we're waiting on outstanding connections
            and confirm which database is affected by logging into the pod and
            querying the PgBouncer admin console.

            ```
            $ kubectl logs -n {{ $labels.namespace }} {{ $labels.instance }} | grep outstanding_connections | tail -n 10
            component=pgbouncer.child event=outstanding_connections database=postgres count=6
            $ kubectl -n {{ $labels.namespace }} exec -it {{ $labels.instance }} bash
            postgres:/$ psql -p 6432 -U pgbouncer pgbouncer -h /tmp
            psql (11.2 (Ubuntu 11.2-1.pgdg18.04+1), server 1.9.0/bouncer)
            Type "help" for help.

            pgbouncer=# show databases;
            ...
            ```

      - alert: StolonPgBouncerStaleStore
        # Ensure we don't alert when shutdown_seconds > 0, as we'll have torn
        # down our etcd listeners once we begin shutdown.
        expr: >
          max by (instance, namespace, version) (
            stolon_cluster_identifier * ignoring(cluster_name, component) (
              (time() - stolon_pgbouncer_store_last_update_seconds) > (6 * stolon_pgbouncer_store_poll_interval and stolon_pgbouncer_shutdown_seconds == 0)
            )
          )
        labels:
          severity: critical
        annotations:
          summary: Stale clusterdata updates from etcd on {{ $labels.instance }}
          dashboard: *dashboardStolonPgBouncer
          description: |

            stolon-pgbouncer watches and polls the etcd store every poll
            interval seconds. This alert is firing if more than 6x the poll
            interval has elapsed since we last received a value from our store.

            This likely means our etcd connection has malfunctioned, or we're
            failing to poll etcd. Checking the pod logs will show any poll
            errors (polling retries continuously) and should hint at what's gone
            wrong.

            ```
            $ kubectl logs -f -n {{ $labels.namespace }} {{ $labels.instance }}
            ```

      - alert: StolonPgBouncerStaleReload
        # Find where the last time we successfully reloaded PgBouncer was
        # greater than 5s behind the last updated keeper value. We'll fire this
        # alert whenever PgBouncer is failing to handle reloads.
        #
        # As with StolonPgBouncerStaleStore, don't alert when we're shutting
        # down as we expect to stop listening to store updates on shutdown.
        expr: >
          max by (instance, namespace, version) (
            stolon_cluster_identifier * ignoring(cluster_name, component) group_right(cluster_name) (
              stolon_pgbouncer_last_keeper_seconds - ignoring(keeper) stolon_pgbouncer_last_reload_seconds > 5
            )
          )
        labels:
          severity: critical
        annotations:
          summary: Failed to reload PgBouncer on {{ $labels.instance }}
          dashboard: *dashboardStolonPgBouncer
          description: |

            stolon-pgbouncer tries reloading PgBouncer in response to keeper
            changes. This alert fires whenever we've seen a new primary keeper
            but have not indicated that PgBouncer has been reloaded with the new
            value.

            It's likely that PgBouncer has hung, or maybe the configuration file
            is invalid and cannot be reloaded. First check pod logs for errors,
            failing that exec into the pod to check the pgbouncer.ini file and
            state of the PgBouncer process.

            ```
            $ kubectl -n {{ $labels.namespace }} exec -it {{ $labels.instance }} bash
            postgres:/$ ps auxf
            ... # expect to see PgBouncer and stolon-pgbouncer
            postgres:/$ psql -p 6432 -U pgbouncer pgbouncer -h /tmp
            psql (11.2 (Ubuntu 11.2-1.pgdg18.04+1), server 1.9.0/bouncer)
            Type "help" for help.

            pgbouncer=# show databases;
            ...
            ```

  - name: stolon-keeper.rules
    rules:
      - alert: StolonKeeperStaleSync
        expr: >
          stolon_cluster_identifier * ignoring(cluster_name, component) group_right(cluster_name) (
            time() - stolon_keeper_last_sync_success_seconds
          ) > 120
        labels:
          severity: critical
        annotations:
          summary: ">1m since successful keeper sync on {{ $labels.instance }}"
          dashboard: &dashboardStolonKeeper 0LgcjLRZz
          description: |

            stolon-keeper periodically attempts to sync the local Postgres to
            the state it receives from the store. This alert will fire when it
            has been more than 2m since the keeper successfully completed a
            sync.

            Check the keeper logs to identify the problem.

      - alert: StolonKeeperRequiresRestart
        expr: >
          stolon_cluster_identifier * ignoring(cluster_name, component) group_right(cluster_name) (
            stolon_keeper_needs_restart == 1
          )
        labels:
          severity: critical
        annotations:
          summary: Postgres is pending restart
          dashboard: *dashboardStolonKeeper
          description: |

            stolon-keeper manages Postgres configuration, along with Postgres
            reload and restarts. This alert is firing when a keeper is reporting
            a required restart that it's been unable to automatically apply.
