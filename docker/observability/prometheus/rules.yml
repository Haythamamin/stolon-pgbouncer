---
groups:
  - name: stolon-pgbouncer.rules
    rules:
      - alert: StolonPgBouncerMultipleMasters
        # Count the number of distinct (cluster_name, keeper) label pairs we
        # have, then roll-up by cluster_name. This tells us if the same cluster
        # is pointing at multiple keepers.
        expr: >
          count by (cluster_name) (
            count by (cluster_name, keeper) (
              stolon_pgbouncer_cluster_identifier * ignoring(cluster_name, store_prefix, keeper) group_left(keeper) stolon_pgbouncer_host_hash
            )
          ) > 1
        labels:
          severity: critical
        annotations:
          summary: PgBouncers in this cluster are pointed at multiple masters
          description: |

            The {{ $labels.cluster_name }} cluster has PgBouncers that are
            pointed at more than one keeper. We only ever try to point at the
            master, and we don't support multiple masters, so this means
            something has gone astray.

            Jump into the dashboard to figure out which pod has gone wrong- it
            will likely be highlighted in red, and won't have reloaded PgBouncer
            in some time.

            Connections via this PgBouncer won't currently be functioning, so
            your first action should be to kill the pod. If new pods come back
            and immediately point elsewhere then you need to debug etcd or the
            cluster itself.

      - alert: StolonPgBouncerPendingShutdown
        expr: >
          max by (pod_name, namespace, version) (
            stolon_pgbouncer_cluster_identifier * ignoring(cluster_name, store_prefix) (
              time() - (stolon_pgbouncer_shutdown_seconds > 0)
            ) > 180
          )
        labels:
          severity: warning
        annotations:
          summary: PgBouncer is pending shutdown on {{ $labels.pod_name }}
          description: |

            stolon-pgbouncer has been sent a SIGTERM and is attempting
            to shutdown, but has been unable to. If this is not handled before
            the termination grace period expires (usually 3hrs) then connections
            will be terminated, causing interruption of service.

            This will happen if you deploy new PgBouncer pods and fail to roll
            the application deployments that use session pooled connections
            through them. If you haven't yet rolled services like
            payments-service, this should be your first action.

            Tail the pod logs to check we're waiting on outstanding connections
            and confirm which database is affected by logging into the pod and
            querying the PgBouncer admin console.

            ```
            $ kubectl logs -n {{ $labels.namespace }} {{ $labels.pod_name }} | grep outstanding_connections | tail -n 10
            component=pgbouncer.child event=outstanding_connections database=postgres count=6
            $ kubectl -n {{ $labels.namespace }} exec -it {{ $labels.pod_name }} bash
            postgres:/$ psql -p 6432 -U pgbouncer pgbouncer -h /tmp
            psql (11.2 (Ubuntu 11.2-1.pgdg18.04+1), server 1.9.0/bouncer)
            Type "help" for help.

            pgbouncer=# show databases;
            ...
            ```
      - alert: StolonPgBouncerStaleReload
        # Ensure we don't alert when shutdown_seconds > 0, as we expect not to
        # reload PgBouncer when we're shutting down. Pending shutdowns should be
        # caught by the StolonPgBouncerPendingShutdown alert, not this.
        expr: >
          max by (pod_name, namespace, version) (
            stolon_pgbouncer_cluster_identifier * ignoring(cluster_name, store_prefix) (
              (time() - stolon_pgbouncer_last_reload_seconds) > (2 * stolon_pgbouncer_store_poll_interval and stolon_pgbouncer_shutdown_seconds == 0)
            )
          )
        labels:
          severity: critical
        annotations:
          summary: Failed to reload PgBouncer on {{ $labels.pod_name }}
          description: |

            stolon-pgbouncer tries to reload PgBouncer every store poll interval
            seconds, but this pod has failed to do so for more than 2x this
            interval.

            Either our connection to etcd is broken or PgBouncer is
            unresponsive, either of which mean database failover won't update
            this PgBouncer.

            Logging into the pod and checking the live processes can show
            whether a PgBouncer has died, and accessing the console might help
            determine the route of the problem.

            ```
            $ kubectl -n {{ $labels.namespace }} exec -it {{ $labels.pod_name }} bash
            postgres:/$ ps auxf
            ... # expect to see PgBouncer and stolon-pgbouncer
            postgres:/$ psql -p 6432 -U pgbouncer pgbouncer -h /tmp
            psql (11.2 (Ubuntu 11.2-1.pgdg18.04+1), server 1.9.0/bouncer)
            Type "help" for help.

            pgbouncer=# show databases;
            ...
            ```
